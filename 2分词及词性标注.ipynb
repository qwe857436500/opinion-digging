{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文分词，词性标注，命名实体识别\n",
    "使用哈工大本地pyltp进行分词，词性标注，命名实体识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyltp import Postagger\n",
    "from pyltp import Segmentor\n",
    "from pyltp import SentenceSplitter\n",
    "import jieba.posseg as pseg\n",
    "import time\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分句\n",
    "def sentenceSplit(sent):\n",
    "    sents = SentenceSplitter.split(sent)\n",
    "    return '_'.join(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词性标注\n",
    "def get_postags(text_list):\n",
    "    postags = []\n",
    "    postagger = Postagger()\n",
    "    postagger.load('F:/model/3.4/pos.model')\n",
    "    for text in text_list:\n",
    "        postag = postagger.postag(text)\n",
    "        postags.append(list(postag))\n",
    "    postagger.release()\n",
    "    return postags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词\n",
    "def segment(filename):\n",
    "    segmentor = Segmentor()\n",
    "    segmentor.load('F:/model/3.4/cws.model')\n",
    "    seg_words = []\n",
    "    with open(filename,mode='r',encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            text = line.strip().replace('\\n','')\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "            try:\n",
    "                text = sentenceSplit(text)\n",
    "                text = text.split('_')\n",
    "                cur_words = []\n",
    "                for t in text:\n",
    "                    words = segmentor.segment(t)\n",
    "                    cur_words.extend(list(words))\n",
    "                seg_words.append(cur_words)\n",
    "            except:\n",
    "                print(error)\n",
    "    segmentor.release()\n",
    "    return seg_words\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除标点符号的分词和词性标注\n",
    "def pos_word_process(seg_list, pos_list):\n",
    "    new_seg_list = []\n",
    "    new_pos_list = []\n",
    "    posDict = {}\n",
    "    for words, poses in zip(seg_list,pos_list):\n",
    "        cur_words = []\n",
    "        cur_pos = []\n",
    "        for word, pos in zip(words,poses):\n",
    "            if pos =='wp':\n",
    "                continue\n",
    "            cur_words.append(word)\n",
    "            cur_pos.append(pos)\n",
    "            posDict[word] = pos\n",
    "        new_seg_list.append(cur_words)\n",
    "        new_pos_list.append(cur_pos)\n",
    "    return new_seg_list, new_pos_list, posDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存分词和词性标注的结果\n",
    "def saveList(filename,obj):\n",
    "    with open(filename, mode='w', encoding='UTF-8') as f:\n",
    "        for ele in obj:\n",
    "            if isinstance(ele, collections.Iterable):\n",
    "                ele = ' '.join(ele)\n",
    "                f.write(ele+'\\n')\n",
    "            else:\n",
    "                f.write(str(ele)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#命名实体识别，接受分词和词性标注参数\n",
    "from pyltp import NamedEntityRecognizer\n",
    "def Ner(word_list, pos_list):\n",
    "    recognizer = NamedEntityRecognizer()\n",
    "    recognizer.load('F:/model/3.4/ner.model')\n",
    "    namedEntity = set()\n",
    "    n = 1\n",
    "    for words, poses in zip(word_list, pos_list):\n",
    "        print('ner',n)\n",
    "        n += 1\n",
    "        try:\n",
    "            if not words:\n",
    "                continue\n",
    "            netags = recognizer.recognize(words,poses)\n",
    "            for id, tags in enumerate(netags):\n",
    "                if tags != 'O':\n",
    "                    namedEntity.add(words[id])\n",
    "            \n",
    "        except:\n",
    "            print(error)\n",
    "    recognizer.release()\n",
    "    return list(namedEntity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存命名实体识别的结果\n",
    "def saveSet(filename, obj):\n",
    "    with open(filename, mode='w',encoding='UTF-8') as f:\n",
    "        for ele in obj:\n",
    "            f.write(ele + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    seg_list = segment('F:/data/comment_clean.txt')\n",
    "    pos_list = get_postags(seg_list)\n",
    "    new_seg_list, new_pos_list, posDict = pos_word_process(seg_list,pos_list)\n",
    "    saveList('F:/data3/comments_clean_seg.txt', new_seg_list)\n",
    "    saveList('F:/data3/comments_clean_pos.txt',new_pos_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner 1\n",
      "ner 2\n",
      "ner 3\n",
      "ner 4\n",
      "ner 5\n",
      "ner 6\n",
      "ner 7\n",
      "ner 8\n",
      "ner 9\n",
      "ner 10\n",
      "ner 11\n",
      "ner 12\n",
      "ner 13\n",
      "ner 14\n",
      "ner 15\n",
      "ner 16\n",
      "ner 17\n",
      "ner 18\n",
      "ner 19\n",
      "ner 20\n",
      "ner 21\n",
      "ner 22\n",
      "ner 23\n",
      "ner 24\n",
      "ner 25\n",
      "ner 26\n",
      "ner 27\n",
      "ner 28\n",
      "ner 29\n",
      "ner 30\n",
      "ner 31\n",
      "ner 32\n",
      "ner 33\n",
      "ner 34\n",
      "ner 35\n",
      "ner 36\n",
      "ner 37\n",
      "ner 38\n",
      "ner 39\n",
      "ner 40\n",
      "ner 41\n",
      "ner 42\n",
      "ner 43\n",
      "ner 44\n",
      "ner 45\n",
      "ner 46\n",
      "ner 47\n",
      "ner 48\n",
      "ner 49\n",
      "ner 50\n",
      "ner 51\n",
      "ner 52\n",
      "ner 53\n",
      "ner 54\n",
      "ner 55\n",
      "ner 56\n",
      "ner 57\n",
      "ner 58\n",
      "ner 59\n",
      "ner 60\n",
      "ner 61\n",
      "ner 62\n",
      "ner 63\n",
      "ner 64\n",
      "ner 65\n",
      "ner 66\n",
      "ner 67\n",
      "ner 68\n",
      "ner 69\n",
      "ner 70\n",
      "ner 71\n",
      "ner 72\n",
      "ner 73\n",
      "ner 74\n",
      "ner 75\n",
      "ner 76\n",
      "ner 77\n",
      "ner 78\n",
      "ner 79\n",
      "ner 80\n",
      "ner 81\n",
      "ner 82\n",
      "ner 83\n",
      "ner 84\n",
      "ner 85\n",
      "ner 86\n",
      "ner 87\n",
      "ner 88\n",
      "ner 89\n",
      "ner 90\n",
      "ner 91\n",
      "ner 92\n",
      "ner 93\n",
      "ner 94\n",
      "ner 95\n",
      "ner 96\n",
      "ner 97\n",
      "ner 98\n",
      "ner 99\n",
      "ner 100\n",
      "ner 101\n",
      "ner 102\n",
      "ner 103\n",
      "ner 104\n",
      "ner 105\n",
      "ner 106\n",
      "ner 107\n",
      "ner 108\n",
      "ner 109\n",
      "ner 110\n",
      "ner 111\n",
      "ner 112\n",
      "ner 113\n",
      "ner 114\n",
      "ner 115\n",
      "ner 116\n",
      "ner 117\n",
      "ner 118\n",
      "ner 119\n",
      "ner 120\n",
      "ner 121\n",
      "ner 122\n",
      "ner 123\n",
      "ner 124\n",
      "ner 125\n",
      "ner 126\n",
      "ner 127\n",
      "ner 128\n",
      "ner 129\n",
      "ner 130\n",
      "ner 131\n",
      "ner 132\n",
      "ner 133\n",
      "ner 134\n",
      "ner 135\n",
      "ner 136\n",
      "ner 137\n",
      "ner 138\n",
      "ner 139\n",
      "ner 140\n",
      "ner 141\n",
      "ner 142\n",
      "ner 143\n",
      "ner 144\n",
      "ner 145\n",
      "ner 146\n",
      "ner 147\n",
      "ner 148\n",
      "ner 149\n",
      "ner 150\n",
      "ner 151\n",
      "ner 152\n",
      "ner 153\n",
      "ner 154\n",
      "ner 155\n",
      "ner 156\n",
      "ner 157\n",
      "ner 158\n",
      "ner 159\n",
      "ner 160\n",
      "ner 161\n",
      "ner 162\n",
      "ner 163\n",
      "ner 164\n",
      "ner 165\n",
      "ner 166\n",
      "ner 167\n",
      "ner 168\n",
      "ner 169\n",
      "ner 170\n",
      "ner 171\n",
      "ner 172\n",
      "ner 173\n",
      "ner 174\n",
      "ner 175\n",
      "ner 176\n",
      "ner 177\n",
      "ner 178\n",
      "ner 179\n",
      "ner 180\n",
      "ner 181\n",
      "ner 182\n",
      "ner 183\n",
      "ner 184\n",
      "ner 185\n",
      "ner 186\n",
      "ner 187\n",
      "ner 188\n",
      "ner 189\n",
      "ner 190\n",
      "ner 191\n",
      "ner 192\n",
      "ner 193\n",
      "ner 194\n",
      "ner 195\n",
      "ner 196\n",
      "ner 197\n",
      "ner 198\n",
      "ner 199\n",
      "ner 200\n",
      "ner 201\n",
      "ner 202\n",
      "ner 203\n",
      "ner 204\n",
      "ner 205\n",
      "ner 206\n",
      "ner 207\n",
      "ner 208\n",
      "ner 209\n",
      "ner 210\n",
      "ner 211\n",
      "ner 212\n",
      "ner 213\n",
      "ner 214\n",
      "ner 215\n",
      "ner 216\n",
      "ner 217\n",
      "ner 218\n",
      "ner 219\n",
      "ner 220\n",
      "ner 221\n",
      "ner 222\n",
      "ner 223\n",
      "ner 224\n",
      "ner 225\n",
      "ner 226\n",
      "ner 227\n",
      "ner 228\n",
      "ner 229\n",
      "ner 230\n",
      "ner 231\n",
      "ner 232\n",
      "ner 233\n",
      "ner 234\n",
      "ner 235\n",
      "ner 236\n",
      "ner 237\n",
      "ner 238\n",
      "ner 239\n",
      "ner 240\n",
      "ner 241\n",
      "ner 242\n",
      "ner 243\n",
      "ner 244\n",
      "ner 245\n",
      "ner 246\n",
      "ner 247\n",
      "ner 248\n",
      "ner 249\n",
      "ner 250\n",
      "ner 251\n",
      "ner 252\n",
      "ner 253\n",
      "ner 254\n",
      "ner 255\n",
      "ner 256\n",
      "ner 257\n",
      "ner 258\n",
      "ner 259\n",
      "ner 260\n",
      "ner 261\n",
      "ner 262\n",
      "ner 263\n",
      "ner 264\n",
      "ner 265\n",
      "ner 266\n",
      "ner 267\n",
      "ner 268\n",
      "ner 269\n",
      "ner 270\n",
      "ner 271\n",
      "ner 272\n",
      "ner 273\n",
      "ner 274\n",
      "ner 275\n",
      "ner 276\n",
      "ner 277\n",
      "ner 278\n",
      "ner 279\n",
      "ner 280\n",
      "ner 281\n",
      "ner 282\n",
      "ner 283\n",
      "ner 284\n",
      "ner 285\n",
      "ner 286\n",
      "ner 287\n",
      "ner 288\n",
      "ner 289\n",
      "ner 290\n",
      "ner 291\n",
      "ner 292\n",
      "ner 293\n",
      "ner 294\n",
      "ner 295\n",
      "ner 296\n",
      "ner 297\n",
      "ner 298\n",
      "ner 299\n",
      "ner 300\n",
      "ner 301\n",
      "ner 302\n",
      "ner 303\n",
      "ner 304\n",
      "ner 305\n",
      "ner 306\n",
      "ner 307\n",
      "ner 308\n",
      "ner 309\n",
      "ner 310\n",
      "ner 311\n",
      "ner 312\n",
      "ner 313\n",
      "ner 314\n",
      "ner 315\n",
      "ner 316\n",
      "ner 317\n",
      "ner 318\n",
      "ner 319\n",
      "ner 320\n",
      "ner 321\n",
      "ner 322\n",
      "ner 323\n",
      "ner 324\n",
      "ner 325\n",
      "ner 326\n",
      "ner 327\n",
      "ner 328\n",
      "ner 329\n",
      "ner 330\n",
      "ner 331\n",
      "ner 332\n",
      "ner 333\n",
      "ner 334\n",
      "ner 335\n",
      "ner 336\n",
      "ner 337\n",
      "ner 338\n",
      "ner 339\n",
      "ner 340\n",
      "ner 341\n",
      "ner 342\n",
      "ner 343\n",
      "ner 344\n",
      "ner 345\n",
      "ner 346\n",
      "ner 347\n",
      "ner 348\n",
      "ner 349\n",
      "ner 350\n",
      "ner 351\n",
      "ner 352\n",
      "ner 353\n",
      "ner 354\n",
      "ner 355\n",
      "ner 356\n",
      "ner 357\n",
      "ner 358\n",
      "ner 359\n",
      "ner 360\n",
      "ner 361\n",
      "ner 362\n",
      "ner 363\n",
      "ner 364\n",
      "ner 365\n",
      "ner 366\n",
      "ner 367\n",
      "ner 368\n",
      "ner 369\n",
      "ner 370\n",
      "ner 371\n",
      "ner 372\n",
      "ner 373\n",
      "ner 374\n",
      "ner 375\n",
      "ner 376\n",
      "ner 377\n",
      "ner 378\n",
      "ner 379\n",
      "ner 380\n",
      "ner 381\n",
      "ner 382\n",
      "ner 383\n",
      "ner 384\n",
      "ner 385\n",
      "ner 386\n",
      "ner 387\n",
      "ner 388\n",
      "ner 389\n",
      "ner 390\n",
      "ner 391\n",
      "ner 392\n",
      "ner 393\n",
      "ner 394\n",
      "ner 395\n",
      "ner 396\n",
      "ner 397\n",
      "ner 398\n",
      "ner 399\n",
      "ner 400\n",
      "ner 401\n",
      "ner 402\n",
      "ner 403\n",
      "ner 404\n",
      "ner 405\n",
      "ner 406\n",
      "ner 407\n",
      "ner 408\n",
      "ner 409\n",
      "ner 410\n",
      "ner 411\n",
      "ner 412\n",
      "ner 413\n",
      "ner 414\n",
      "ner 415\n",
      "ner 416\n",
      "ner 417\n",
      "ner 418\n",
      "ner 419\n",
      "ner 420\n",
      "ner 421\n",
      "ner 422\n",
      "ner 423\n",
      "ner 424\n",
      "ner 425\n",
      "ner 426\n",
      "ner 427\n",
      "ner 428\n",
      "ner 429\n",
      "ner 430\n",
      "ner 431\n",
      "ner 432\n",
      "ner 433\n",
      "ner 434\n",
      "ner 435\n",
      "ner 436\n",
      "ner 437\n",
      "ner 438\n",
      "ner 439\n",
      "ner 440\n"
     ]
    }
   ],
   "source": [
    "namedEntity = Ner(new_seg_list, new_pos_list)\n",
    "saveSet('F:/data3/comments_clean_ner.txt', namedEntity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算词频\n",
    "def countFrequency(words_list):\n",
    "    wordFreq = {}\n",
    "    for words in words_list:\n",
    "        for word in words:\n",
    "            if wordFreq.get(word) == None:\n",
    "                wordFreq[word] = 1\n",
    "            wordFreq[word] += 1\n",
    "    return wordFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存词频，词性标注字典\n",
    "def saveDict(filename, obj):\n",
    "    with open(filename, mode='w', encoding='UTF-8') as f:\n",
    "        for left, right in obj.items():\n",
    "            f.write(left)\n",
    "            f.write(':')\n",
    "            if isinstance(right, collections.Iterable):\n",
    "                right = ','.join(list(right))\n",
    "                f.write(right+'\\n')\n",
    "            else:\n",
    "                f.write(str(right)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_seg_list[0][0] = new_seg_list[0][0].replace('\\ufeff','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFreq = countFrequency(new_seg_list)\n",
    "saveDict('F:/data3/comments_clean_posDict.txt',posDict)\n",
    "saveDict('F:/data3/comments_clean_wordFreq.txt',wordFreq)\n",
    "with open('F:/data3/comments_clean_posDict.pkl', 'wb+') as f:\n",
    "    pickle.dump(posDict, f)\n",
    "with open('F:/data3/comments_clean_wordFreq.pkl', 'wb+') as f:\n",
    "    pickle.dump(wordFreq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
